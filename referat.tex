\documentclass[14pt, a4paper,oneside]{extarticle}
\usepackage[T1,T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{epstopdf}
\usepackage{verbatim}
\usepackage[linesnumbered,boxed]{algorithm2e}
\usepackage[]{graphicx}
\usepackage{setspace}

\usepackage{indentfirst}
\usepackage{hyperref} %url

\usepackage{amsmath} 
 \setcounter{tocdepth}{2} 
\usepackage{floatrow}
% Table float box with bottom caption, box width adjusted to content
\newfloatcommand{capbtabbox}{table}[][\FBwidth]

\onehalfspacing

%\graphicspath{{image/}}
\usepackage[left=2cm,right=2cm,bottom=3cm,top=2cm]{geometry}
\bibliographystyle{unsrt}
\SetKwProg{Op}{Operator}{}{}
\SetKwProg{Func}{Function}{}{}
\SetAlgoNlRelativeSize{0}
\author{Швецов Денис}
\title{Реферат по английскому}
\begin{document}
\thispagestyle{empty}

	\thispagestyle{empty}
	
	\begin{center}
		\ \vspace{-0.5cm}
		
		\includegraphics[width=0.5\textwidth]{msu}\\
		{Московский государственный университет имени М.В. Ломоносова}\\
		Факультет вычислительной математики и кибернетики\\
		Кафедра автоматизации систем вычислительных комплексов
		
		\vspace{2.5cm}
		
		{\Large ШВЕЦОВ Денис Андреевич}
		
		\vspace{1cm}
		
		{\Large\bfseries
			Алгоритмы управления перегрузками в центрах обработки данных\\}
		
		\vspace{2cm}
		
		{\large РЕФЕРАТ}
	\end{center}
	\vfill
	\begin{center}
		Москва, 2017
	\end{center}
	\vspace{1cm}
	\enlargethispage{4\baselineskip}

\newpage
	\addcontentsline{toc}{section}{Оглавление}
	\tableofcontents
\newpage
\section{Введение}

На сегодняшний день центры обработки данных являются важнейшей частью инфраструктуры организаций предоставляющие всевозможные он-лайн сервисы для своих клиентов, например, веб-поиск, торговые площадки, рекомендательные системы. Качество работы этих сервисов напрямую влияет на количество заинтересованных пользователей и следственно на доход. 

Приложения он-лайн сервисов должны работать в реальном времени~--- пользователь вводит запрос в веб-браузере и ожидает незамедлительного отклика (время отклика не должно превышать 300 миллисекунд). 
Так же приложения работают с большими объемами данных (например, весь веб-индекс) для формирования ответа на запрос. Обычно эти данных распределены между тысячами сервером и каждый запрос попадает на все сервера.

Высокие нагрузки на центры обработки данных и жесткие требования приложений порождают жесткие требования на сети в центрах обработки данных. Работа приложений в реальном времени требует низких задержек в сети и устойчивости к всплескам сетевой активности. Также поскольку приложениям надо постоянно обновлять внутренние структуры данных сети должны иметь высокую пропускную способность для длительных потоков. При этом, несмотря на предъявленные требования, в сетях центров обработки данных зачастую используется дешевое сетевое оборудование, например, ToR (Top of the rack) коммутатор, который соединяет между собой сервера в стойке, имеет 48 портом скоростью 1 гигабит в секунду.

В условиях сетей центров обработки данных TCP протокол не достаточно хорошо справляется с поставленными требованиями~\cite{dctcp}, поэтому ведутся работы по разработке протоколов управления перегрузками в центрах обработки данных.
Cуществуют подходы, в которых используются возможности протокола TCP, такие как ECN метка~\cite{dctp, d2tcp}, при этом остальные функции протокола остаются нетронутыми.
Параллельно с этим существуют работы, в которых, разрабатывают протоколы не совместимые с TCP~\cite{d3tcp}, например, Facebook разработала свой протокол управления перегрузками поверх UDP~\cite{facebook}.

\newpage

\section{Центры обработки данных}
\subsection{Приложения центров обработки данных}

На сегодняшний день крупномасштабные приложения в центрах обработки данных используют разработаны в соответствии с подходом разделение/агрегация (см.~рис.~\ref{dc_arch})
Запрос с уровней, расположенных наверху, делится на части и передаются на нижний уровень обрабатывающем процессам. Ответ от этих обрабатывающих процессов агрегируется для получения результирующего ответа. Такой подход используется даже в таких сервисах обработки данных как MapReduce и  Dryad . % Может быть и тут ссылки вставаить
\begin{figure}[h]
	\includegraphics[width=0.7\linewidth]{datacenter_arch}
	\caption{Архитектура разделение/агрегация}
	\label{dc_arch}
\end{figure}

Интерактивная природа веб-приложений означает, что задержка это значимый параметр. Приложения обычно имеют ограничения в 200-300 миллисекунд для того, чтобы завершить свои вычисления и вернуть ответ пользователю~\cite{sla}. Это ограничение ведет к тому приводят к ограничениям на задержку на каждом уровне иерархии центров обработки данных. Эти ограничения означают, что сетевые потоки, которые несут в себе запросы и ответы к узлам и от них, имеет директивные интервалы выполнения, и если какой-то узел не уложился в свой директивный интервал, вычисления продолжаются без его ответа, что снижает качество результата, не говоря уже о бесполезном использовании пропускной способности сети.

\subsection{Сетевая нагрузка в центрах обработки данных}
Были проведены исследования о том, какой трафик существует в центрах обработки данных~\cite{dctcp}, были выделены три различных типов трафика:
\begin{enumerate}
\item трафик запросов
\item трафик коротких сообщений, который координирует активность в центре обработки данных
\item фоновый трафик, который передает большие объемы данных между серверами для того, чтобы приложения поддерживали внутренние структуры в актуальном состоянии и выдали ответы высокого качества.
\end{enumerate}

\paragraph{Трафик запросов.}
Трафик запросов использует модель разделения агрегации и состоит из очень коротких потоков, критичных к задержке. Агретор высшего уровня разделяет запрос на большое количество агрегаторов среднего уровня, которые в свою очередь деляет каждый запрос на 43 других сервера, находящийся в той же самой стойке. Размеры трафика запросов достаточно постоянный~--- 1,6 килобайт для запросов и 1,6-2 килбоайта для ответов.
% Можно еще припихнуть картинку про распределение времени между двумя запросами

\paragraph{Фоновый трафик.}
\begin{figure}
\includegraphics[width=\linewidth]{pdf_of_flow_size.png}
\caption{Плотность вероятности размера потока для фонового трафика. Плотность вероятности общего количество данных показывает вероятность того, что случайно выбранный байт будет принадлежать потоку данного размера.}
\label{pdf_of_flow_size}
\end{figure}

Параллельно с трафиком запросов существует фоновый трафик, состоящий и из больших и из маленьких потоков. % Плостность распредления потоков
На рисунке~\ref{pdf_of_flow_size} видно, что большинство фоновых потоков имеет маленький размер, но большинство данных в фоновом трафике принадлежит большим потокам.
%TODO Дописать эту часть

\subsection{Проблемы сетей в центрах обработки данных}

Большинство коммутаторов в центрах обработки данных это \emph{коммутаторы с разделяемой памятью}, использующие статического мультиплексирования с помощью буфера пакетов доступного для всех портов коммутатора. Пакет прибывает на интерфейс и сохраняется в высокоскоростную память разделенную между всеми интерфейсами. Устройство управления памятью динамически выделяет для пакета память. Устройство управления памятью старается дать интерфейсу столько памяти, сколько ему необходимо, динамически настраивая максимальный объем памяти, который может использовать любой интерфейс.
Если пакет должен быть поставлен в очередь на отправку на некоторый интерфейс, но этот интерфейс достиг максимума выделенной ему памяти или истощился сам пул свободной разделяемой памяти, тогда пакет будет отброшен. Создание разделяемой памяти большого объема очень дорогой процесс, поэтому большинство дешевых коммутаторов имеют \emph{маленький буфер пакетов}. Этот маленький буфер пакетов является причиной трех специфичных проблем, которые описаны далее.

\paragraph{Incast}

Incast это модель общения в сетях, в которой от многих отправителей пакет адресован одному получателю. Если на коммутаторе много потоков выходят на одном и том же интерфейсе в один и тот же период времени, пакеты могут израсходовать всю выделенную на интерфейс память, что приведет к потере пакетов. Это может произойти даже если потоки небольшие. Такой трафик естественным образом возникает в центрах обработки данных из-за использование модели разделение/агрегация, запросы поступают на обрабатывающие процессы в одно и то же время, и процессы отвечают примерно в тоже время, что и создает проблему Incast трафика.

При этом так как размер каждого отдельного ответа приложения достаточно мал (в проведенном исследовании~\cite{dctcp} он составляет 2 килобайта, что умещается в два пакета) потеря пакета почти не оказывает влияния на время ожидания переотправления пакета в TCP. Если выставить минимальное время ожидания перепосылки пакета равным одному RTT это будет примерно 300 миллисекунд, следовательно если случается потеря пакета ответ приложения практически всегда не укладывается в свой директивный интервал.

Разработчики сделали два основных изменения а коде приложений, чтобы избежать тайм-аутов для ответов обрабатывающих процессов.
\begin{enumerate}
\item во-первых они явно ограничили размер ответ двумя килобайтами, чтобы улучшить шансы того, что все ответы уместятся в памяти коммутатора,
\item во-вторых разработчики добавили джитеринг~\cite{jittering} в уровень приложений, чтобы десинхронизовать ответы приложений с помощью задержки ответа приложения на некоторое случайное время, или использовали слоттирование времени ответа для тех же целей~\cite{ictcp}.
\end{enumerate}
%То, что уменьшение RTO_min не помогает

\paragraph{Загрузка очереди.}
Длительные и большие TCP потоки могут являться причиной того, что длина очереди в узком месте на коммутаторе будет расти до тех пор, пока пакет не начнут отбрасываться приводя к <<пилообразному>> процессу изменения скорости передачи (см рис.~\ref{tcp_sawtooth}).
\begin{figure}
	\includegraphics[width=0.7\linewidth]{tcp_sawtooth.jpg}
	\caption{Пилообразный график скорости передачи TCP}
	\label{tcp_sawtooth}
\end{figure}
Учитывая тот факт, что длительные и короткие потоки обрабатываются в одной и той же очереди, можно выделить две проблемы. Во-первых, потеря пакетов в коротких потоках может привести к проблеме Incast трафика. Во-вторых, даже если потери пакетов не было, короткие потоки испытывает б\'{о}льшие задержки, так как они находятся в очереди за пакетами длительных и больших потоков. Так как каждый обрабатывающий процесс в центре обработки данных работает и с трафиком запросов и с фоновым трафиком, такое состояние может случаться довольно часто.

Исследования~\cite{dctcp} показали, что в центрах обработки данных с классическим TCP, короткие потоки в самом деле испытывают неприемлимые задержки из-за длинной очереди на коммутаторе (до 14 миллисекунд). При этом потери пакетов не было, так что единственный способ борьбы с такой задержкой  это уменьшение очереди.

\paragraph{Переполнение буфера.}
В центре обработке данных сосуществует смесь длительных и коротких потоков, и очень частая ситуация, когда короткие потоки на одном порту испытывают влияние активностью на любом из других портов. Коэффициент потерь коротких потоков зависит от числа длительных потоков, проходящих через другие порты. Это происходит из-за того, что активность на разных портах разделяет между собой общую память.

Длительные и жадные TCP потоки заполняют очередь на своих интерфейсах. Так как пространство буфера это разделяемый ресурс, заполнение очереди уменьшает доступное пространство памяти для того справится с всплесками потоков. В результате происходит потеря пакетов, так же как и в проблема Incast трафика, но даже с рассинхронными потоками.

\subsection{Директивные интервалы}

TCP не использует тот факт, что у приложений в центрах обработки данных, имеют сроки выполнения, и разделяет пропускную способность равномерно между всеми потоками, игнорируя их директивные интервалы. Использование знания о директивных интервалах потоков может помочь двумя способами~\cite{d3tcp}:
\begin{enumerate}
\item \emph{<<несправедливо>> разделив пропускную способность}: 
Предположим, что два потока, разделяют между собой один канал связи, который является узким местом, один из этих потоков имеет более сжатый директивный интервал, чем другой. Протокол TCP старается разделить пропускную способность поровну между потоками. Еа рисунке~\ref{fairness_two_flow} показан пример двух потоков с разными директивными интервалами, только один поток укладывается в свой директивный интервал и только он будет принят для ответа пользователю. 
\begin{figure}
	\includegraphics[width=0.7\linewidth]{fairness_two_flow}
	\caption{Два потока $f1, f2$ с различными директивными интервалами $(d1, d2$). Толщина линии обозначает пропускную способность выделенную для потока. Учитывая директивные интервалы, можно добиться того, чтобы оба потока укладывались в них}
	\label{fairness_two_flow}
\end{figure}
При этом поток, ответ которого не будет учитываться все равно будет занимать ресурсы сети. С другой стороны, если учитывать информацию о директивных интервалах потоков можно неравномерно распределить пропускную способность, чтобы оба потока успели закончиться в срок.

\item \emph{подавляя некоторые потоки:}
Предположим, что одно распределенное приложение использует несколько серверов, потоки содержащие ответы имеют одинаковые директивные интервалы. Эти потоки разделяют между собой единый канал связи. В предположении, что пропускная способность этого канала связи не в состоянии закончить передачу этих потоков, укладываясь в их директивный интервал, ответ никакого из этих серверов не будет принят (пример показан на рисунке~\ref{fairness_many_flows}).
\begin{figure}
	\includegraphics[width=0.7\linewidth]{fairness_many_flows}
	\caption{Несколько потоков с одинаковыми директивными интервалами. Пропускная способность канала не позволяет передать все потоки в их директивных интервал. Если подавить один из потоков, для оставшихся можно будет этого добиться.}
	\label{fairness_many_flows}
\end{figure}
При этом, учитывая директивные интервалы, можно подавить потоки ответов некоторых серверов, для того, чтобы ответы оставшихся серверов были приняты в срок.
\end{enumerate}

Эти два примера показывают, что директивные интервалы играют важную роль в центрах обработки данных. И учитывая их можно повысить качество работы приложений.
\newpage

\section{Алгоритмы управления перегрузками}
\subsection{Data Center TCP (DCTCP)}
Основная цель Data Center TCP (DCTCP)~--- достигнуть высокой пропускной способности, устойчивости к всплескам и низкой задержки в центрах обработки данных на коммутаторах с маленьким буфером. Поэтому при DCTCP поддерживает маленькую загрузку очереди на коммутаторе при той же пропускной способности.

DCTCP достигает поставленных целей напрямую реагируя перегрузке пропорционально ее степени. Для этого используется схема маркирования пакетов на коммутаторах меткой \emph{Congestion Experinced (CE)} как только размер занятного буфер превысил некоторый небольшой порог. DCTCP отправитель реагирует на это, уменьшая TCP окно перегрузки на значение зависящее от доли маркированных пакетов: чем больше доля маркрованных пакетов, тем больше значение, на которое уменьшится окно перегрузки.

Важно заметить, что ключевым компонентом в DCTCP является не правила управления перегрузками как таковые, а то, каким образом многобитовая информация представляется в последовательности однобитовых меток пакетов. И так как DCTCP требует от сети только один бит в пакете для передачи своей информации, можно использовать технологию Explicit Congestion Notification (ECN), которая доступна в большинстве современных коммутаторах и TCP стеках.

Алгоритм DCTCP состоит из трех основных компонент:
\begin{enumerate}

\item
во-первых, маркирование пакетов на коммутаторах.
DCTCP использует очень простую схему управления очередью. Есть только один параметр~--- порог маркирования $K$. Прибывший пакет маркируется CE меткой, если размер занятой очереди больше чем $K$, иначе он не маркируется. Это схема обеспечивает быстрое информирование отправителя о том, что очередь переполняется. Можно адаптировать механизм Random Early Detection (RED)~\cite{red} для DCTCP, достаточно просто выставить нижние и верхние пороги равными K;
\item
во вторых, ответ с ECN меткой на получателе. 
Единственное различие между TCP получателем и DCTCP получателем, заключается в том, каким образом информация о CE метках отправляется обратно на отправителя. RFC 3168 устанавливает, что получатель отправляет набор ECN меток в серии подтверждений полученных пакетов до тех пор, пока он не получит отправитель не сообщит, что эти метки были получены.
В то время как DCTCP получатель пытается передать точную последовательность CE меток отправителю. Самый простой способ сделать это, помечать пакет с подтверждением меткой ECN тогда и только тогда, когда полученный пакет помечен CE меткой.

Тем не менее использовать технику отложенного подтверждения получения пакета (одно подтверждение на несколько полученных пакетов) важно по многим причинам, включая уменьшение нагрузки на отправителя. Чтобы поддерживать отложенное подтверждение DCTCP использует детерминированный конечный автомат с двумя состояниями, для того отправки подтверждения (см. рис.~\ref{state_machine}).
\begin{figure}
\includegraphics[width=0.7\linewidth]{state_machine.png}
\caption{Конечный автомат отправки подтверждения получения пакета}
\label{state_machine}
\end{figure}
Так как отправитель знает, сколько отправленных полученное подтверждение покрывает, он может точно восстановить сколько маркированных пакетов пришло к получателю.

\item
Контролирование окна перегрузок на отправителе.
Отправитель оценивает долю пакетов, которые были помечены, обозначенную за $\alpha$. Данная переменная обновляется через каждое окно переданных данных (равное примерно одному RTT) следующим образом:
\begin{equation} \label{eq:alpha_update}
\alpha \leftarrow (1 - g) \times \alpha + g \times F,
\end{equation}
где $F$ это доля пакетов, которые были помечена в прошлом окне, и $0 < g < 1$ это вес полученной новой информации по отношению к уже установленному значению $\alpha$. При условии, что отправитель получает маркированные пакеты, только когда длина очереди больше чем $K$ и не получает маркированных пакетов, когда длина очереди меньше, выражение~\eqref{eq:alpha_update} означает, что $\alpha$ оценивает вероятность того, что размер очереди больше чем $K$. Значение $\alpha$ близкое у нулю означает низкий уровень перегрузок, в то время как значение $\alpha$ близкое к единице означает высокий уровень перегрузок.

Единственная разница между отправителем в DCTCP и отправителем в TCP, это то, каким образом они реагируют на получение подтверждения, остальные возможности, такие как как медленный старт, аддитивное увеличение для управления перегрузками, или перепосылка потерянных пакетов остаются такими же.
Но, в то время как TCP всегда уменьшает окно перегрузок в два раза в ответ на полученное маркированное подтверждение, DCTCP использует значение $\alpha$
\begin{equation} \label{eq:cwnd}
cwnd \leftarrow cwnd \times (1 - \alpha/2)
\end{equation}
Таким образом, когда значение $\alpha$ близко к нулю (низкий уровень перегрузок), окно перегрузок уменьшается лишь немного. Другими словами, DCTCP отправитель начинает аккуратно уменьшать окно перегрузок сразу же, как размер очереди превысит $K$. Таким образом DCTCP поддерживает маленькую загрузку очереди, при этом сохраняя высокую пропускную способность. Когда же уровень перегрузок высокий ($\alpha = 1$), DCTCP уменьшает окно перегрузок в два раза, так же как и TCP.
\end{enumerate}

Автора DCTCP анализировали параметры своего алгоритма, они исходили из предположения, что существует $N$ бесконечно длительных потоков с одинаковой круговой задержкой $RTT$, которые делят между собой одно соединение вместимостью $C$? и пришли к следующим выводам:
\begin{enumerate}
\item порог маркирования $K > \frac{C \times RTT}{7}$
\item вес новой информации о о доле помечанных пакетов $g < \frac{1.386}{\sqrt{2(C \times RTT + K)}}$
\end{enumerate}

Таким  образом DCTCP отправитель начинает реагировать на перегрузку сразу же, как только длина очереди на интерфейсе превысит порог $K$, это уменьшает задержки в очереди, и минимизирует негативное действие длительных потоков на короткие, так же большее свободное пространство в буфере дает возможность справиться с микровсплесками, без потери пакетов, которые могут привести к таймаутам. Так же DCTCP не позволяет длине очереди на выходном интерфейсе сильно вырасти, так что в коммутаторах с разделяемой памятью перегруженные порты не будут влиять на оставшиеся. 

\subsection{Deadline-Driven Delivery control protocol (D\textsuperscript{3})}

Создатели протокола D\textsuperscript{3}~\cite{d3tcp} пришли к выводу, что использование информации о директивных интервалах, может повысить качество приложений. В работе, посвященной разработке этого протокола, исследователи выдвинули два способа, с помощью которых можно использовать информацию о директивных интервалах в потоках:
\begin{enumerate}
\item \emph{ Earlist Deadline First (EDF)~\cite{edf} }, алгоритм в котором маршрутизаторы приоритезериуют потоки, основываясь на их директивных интервалах~-- чем ближе окончание директивного интервала тем раньше обрабатывается пакет,
\item \emph{резервирование пропускной способности}, поток размером $s$ с директивным интервалом длиной $d$ полностью успевает обработаться с пропускной способностью $r = s/d$.
\end{enumerate} 

В своей работе исследователи поставили протоколом D\textsuperscript{3} следующие цели : максимизировать количество потоков, укладывающихся в свои директивные интервалы, обеспечить устойчивость к всплескам сетевой активности, добиться высокой утилизации сети.

Протокол D\textsuperscript{3} использует подход с резервирование пропускной способности. Отправителю необходимо запросить у сети пропускную способность, необходимую для передачи необходимой информации в директивный интервал. Запрос передает в заголовке пакета, который проходит через все маршрутизаторы на пути следования к пункту назначения. Каждый маршрутизатор назначает выделенную пропускную способность, которая передается отправителю через пакет подтверждения на обратном пути. Отправитель ограничивает скорость передачи данных минимальной из пропускных способностей, выделенных ему маршрутизаторами. Отправитель будет передавать данные на этой скорость в течении времени равного одному RTT, после которого необходимо будет выполнить новый запрос в одном из передаваемых пакетов.

Отправителю необходимо периодически запрашивать сеть о новой пропускной способности, потому что старые потоки могут закончится, что освободит пропускную способность и позволит передавать данные быстрее, или наоборот могут начаться новые потоки. И так как выделенная пропускная способность может быть больше или меньше запрошенной, отправитель каждый раз запрашивает пропускную способность основываясь на актуальной информации о количество данных, оставшихся для передачи и близости окончания директивного интервала.

Каждый отправитель запрашивает пропускную способность, основываясь на своих директивных интервалах, при этом существуют потоки без директивных интервалов (они запрашивают пропускную способность $r = 0$). Можно сформулировать задачу выделения пропускной способности следующим образом: по данным запросам, необходимо выделить пропускную способность потокам таким образом, чтобы максимизировать число потоков, укладывающихся в свои директивные интервалы и при этом полностью утилизировать ресурсы сети.
В условиях динамического планирования эта задача NP-полная~\cite{np-complete}, поэтому исследователи использовали жадный алгоритм выделения пропускной способности. Когда маршрутизатор получает пакет с запросом пропускной способности размером $r$, он стремится выделить хотя бы минимальный размер. Если после выделения пропускной способности для всех потоков имеющих директивные интервалы у маршрутизатора остается невыделенная пропускная способность он разделяют ее поровну между всеми текущими потоками. Следовательно, если пропускная способность роутера больше, чем пропускная способность необходимая для всех потоков, то для каждого потока выделенная пропускная способность $a$ будет определяться следующим образом: 
$$ a = r + fs, $$
где $fs$ это равномерно распределенная пропускная способность, оставшаяся после выделения необходимой.

Тем не менее, в случае, если маршрутизатор не имеет достаточно пропускной способности, чтобы все потоки уложились в свои директивные интервалы, он жадным образом старается обеспечить необходимой пропускной способностью так много потоков, как только сможет. Оставшиеся потоки (имеющие и не имеющие директивные интервалы) получат \emph{базовую пропускную способность}, которая им позволит посылать только заголовки пакетов один раз за время равное одному RTT и, следовательно, запрашивать пропускную способность в будущем.

Описанная схема выделения пропускной способности подразумевает, что маршрутизатор получает запросы на выделение для всех потоков в один и тот же момент времени. В реальности, маршрутизатору необходимо принимать решение динамически~--- все запросы распределены по времени, а потоки постоянно начинаются и заканчиваются. Чтобы достичь этого операции выделения пропускной способности слоттированы (с точки зрения конечных устройств). Выделенная пропускная способность действительна только в промежуток времени текущего слота (который равен одному RTT), после чего потоку необходимо опять запросить пропускную способность. Запрос пропускной способности в момент времени $t$ преследует две цели :
\begin{enumerate}
\item запросить у маршрутизаторов пропускную способность $a_{t+1}$ на время следующего слота,
\item вернуть, занимаю в предыдущем слое пропускную способность $a_t$.
\end{enumerate}

Для достижения первой цели, маршрутизатор следит за тремя переменными:
\begin{enumerate}
\item $N$: число потоков, проходящих через интерфейс (обновляется с помощью специальных TCP пакетов начала и окончания сессии~--- TCP SYN/FIN),
\item $D$: пропускная способность, необходимая для потоков, имеющих директивные интервалы,
\item $A$: общая выделенная пропускная способность.
\end{enumerate}

Чтобы достичь второй цели, маршрутизатору необходимо знать выделенную пропускную способность для потока в предыдущем слоте. Можно было бы просто запоминать эту информацию на самом маршрутизаторе, но учитывая тот факт, что большинство потоков с директивными интервалами очень короткие, такой подход слишком трудоемкий по вычислительным ресурсам и по памяти. Поэтому создатели протокола D\textsuperscript{3} передают эту информацию в пакете, вместе с запросом пропускной способности. То есть в пакете, помимо запроса пропускной способности $r_{t+1}$ в следующем слоте содержится информация о \emph{необходимой} пропускной способности в предыдущем слоте $r_t$, и вектор \emph{выделенной} пропускной способности на каждом маршрутизаторе $[(a_t)]$ на пути следования пакета к пункту назначения.

Протокол D\textsuperscript{3} предоставляет приложение интерфейс, похожи на расширенный интерфейс сокетов, который позволяет задать размер потока и директивные интервалы при создании этого сокета.

Заголовок запроса пропускной способности представлен на рисунке~\ref{header}, схема работы представлена на рисунке~\ref{d3_scheme}.
\begin{figure}
\includegraphics[width=0.7\linewidth]{header}
\caption{Заголовок запроса пропускной способности в протоколе D\textsuperscript{3}.}
\label{header}
\end{figure}
\begin{figure}
\includegraphics[width=0.6\linewidth]{d3_sheme}
\caption{Схема обмена пакетами в протоколе D\textsuperscript{3}. $RRQ$ это запрос пропускной способности.}
\label{d3_sheme}
\end{figure}

Выделяет необходимую пропускную способность приложениям, а оставшуюся разделяет равномерно между потоками, и таким образом достигает высокую утилизацию сети, при этом благодаря явного ограничения скорости передачи данных на отправители выделенной пропускной способности  отсутствуют всплески сетевой активности.

\subsection{Deadline-Aware Datacenter TCP D\textsuperscript{2}TCP}

D\textsuperscript{2}TCP представляет еще один пример использование знаний о директивных интервал в управлении перегрузками в центрах обработки данных. В то время как разработчики D\textsuperscript{3} сконцентрировали свое внимание на передачу информации о директивных интервал в сеть передачи данных и выделили только два способа сделать это~\cite{d3tcp}, разработчики D\textsuperscript{2}TCP использовали другой подход, они использовали информацию о директивных интервалах в отправителе, позволив ему менять скорость изменения окна перегрузок, в зависимости от близости окончания директивного интервала. Такая схема, позволяет использовать этот протокол, не меняя сетевое оборудование.

D\textsuperscript{2}TCP основан на DCTCP, с добавлением в него информации о директивных интервалах. Ожидается, что коммутаторы поддерживают технологию ECN, и сконфигурированы помечать пакеты CE меткой, как только очередь пакетов превысит некоторый определенный порог. D\textsuperscript{2}TCP поддерживает переменную $\alpha$, с помощью уравнения~\eqref{eq:alpha_update}.

Далее определяется переменная $d$, обозначающая близость окончания директивного интервала для потока (более подробно будет описано позже), большое значение $d$ означает, что поток не успевает передать необходимые данные в свой директивный интервал, на основании $\alpha$, и $d$ определяется \emph{функция штрафа p}, применяемая к размеру окна перегрузок W:
\begin{equation}\label{eq:penalty}
p = \alpha^{d}.
\end{equation}
Функция~\eqref{eq:penalty} это функция гамма-коррекции (см.~рисунок~\ref{gamma}), которая часто используется в цветокоррекции в компьютерной графике. Стоит заметить, что, поскольку $\alpha \leq 1$, то и $p \leq 1$. После определения $p$, окно перегрузок меняется следующим образом:
\begin{equation}\label{eq:window_resize}
W = \begin{cases}
W\times(1 - \frac{p}{2}), p > 0\\
W + 1,~~~~~~~~ p = 0
\end{cases}
\end{equation}
\begin{figure}
	\includegraphics[width=0.6\linewidth]{gamma}
	\caption{Функция гамма-коррекции $p = \alpha^d$}
	\label{gamma}
\end{figure}
В случае, если $\alpha$ равняется нулю (то есть не было помеченных пакетов, что означает отсутствие перегрузки) и следовательно $p$ равно нулю, размер окна перегрузок увеличивается на единицу, так как же как и классическом TCP. Когда все пакеты помечены, $\alpha = 1$ и $p = 1$, тогда окно перегрузок уменьшается в два раза, так же как и в TCP. В остальных случаях размер изменения окна модулируется переменной $p$.

Если $d = 1$, то $p = \alpha$, и это приводит к поведению совпадающим с поведением DCTCP. Если $d > 1$, то $p$ медленно увеличивается с ростом $\alpha$, пока $\alpha$ не подойдет близко к единице. Другими словами небольшие перегрузки не сильно влияют на потоки с близким окончанием директивного интервала. Если $d < 1$, то $p$ увеличивается быстро уже на небольших значениях $\alpha$, что означает, что даже при небольших перегрузках происходит быстрое уменьшение окна перегрузок для потоков, чьё окончания директивного интервала наступит не скоро.
Случаи $d > 1$ и $d < 1$ дополняют друг друга при перегрузках.
 
Для того, чтобы определить переменную $d$, используются два параметра потока~--- $T_C$ - время необходимое, для передачи всех данных и $D$ - время, оставшиеся до окончания директивного интервала. Если поток успевает передать свои данные в директивный интервал, то $T_C \approx D$. Если $T_C > D$ необходимо выставить $d > 1$, чтобы указать сжаты сроки передачи данных, и наоборот. Следовательно $d$ можно определить следующим образом.
$$ d = \frac{T_C}{D}$$

Исследователи рассчитали точное значение $T_C$, основываясь на текущем размере окна перегрузки $W$, количестве необходимых для передачи байт $B$, и периоде процесса аддитивного увеличения, мультипликативного уменьшения $L$. Однако они обнаружили, что также можно использовать приближенное значение $T_C$, основываясь на том факте, что в среднем размер окна перегрузки равняется $\frac{3}{4}W$. И следовательно,
$$T_C = B / \big( \frac{3}{4}W \big)$$ 

Поскольку $d$ появляется в степени гамма-функции, очень большие и близкие у нулю значения для $d$ могут привести к тому, что $p$ будет вести себя как бинарная компонента. То есть будут иметь смысл просто наличие перегрузки, вместо значение степени загруженности сети. Поэтому исследователи ограничили $d$ сверху и снизу значениями $0.5$  и $2$ соответственно.

Таким образом D\textsuperscript{2}TCP использует преимущества DCTCP, такие как возможность реализации на большинстве коммутаторов, устойчивость к всплескам, небольшая загрузка очереди. При этом D\textsuperscript{2}TCP использует информацию о директивных интервалах потоков, позволяя потокам с близким окончанием директивного интервала меньше реагировать на небольшую загрузку сети.

\newpage

\section{Оценка алгоритмов}

Создатели DCTCP проанализировали свой продукт, и показали что со случаем DCTCP очередь на коммутаторе намного меньше и постоянее, чем с классическим TCP (см~рис.\ref{queue_size}). 
\begin{figure}
	\includegraphics[width=\linewidth]{queue_size}
	\caption{Загрузка очереди на коммутаторе с динамическим распределением памяти.}
	\label{queue_size}
\end{figure}
На рисунке~\ref{queue_size2} представлена функция распределения вероятности длины очереди (в пакетах) с параметром $K$ равным 20, в соответствии с рекомендациями. В эксперименте использовались коммутатор со портами скоростью 1 Гб/с, в результате было выявлено, что DCTCP достиг максимульной пропускной способности 0.95 Гб/c, а утилизация канала близка с 100\%. Но по сравнению с TCP длина очереди устанавливается в районе 20 пакетов, что более чем в 10 раз меньше, чем длина очереди TCP.
\begin{figure}
	\includegraphics[width=0.5\linewidth]{queue_size2}
	\caption{Функция распределения вероятности для длины очереди (1Гб/c)}
	\label{queueu_size2}
\end{figure}

Так же исследователи измерили пропускную способность в зависимости от параметра $K$ на соединениях скоростью 10Гб/c (результаты представлены на рисунке~\ref{throughput}). Было обнаружено, что когда $K$ превышает рекомендованное значение 65 DCTCP работает с той же пропускной способностью, что и TCP, и больше не чувствителен к параметру $K$.
\begin{figure}
	\includegraphics[width=0.5\linewidth]{throughput}
	\caption{Пропускная способность (10 Гб/с)}
	\label{throughput}
\end{figure}

Исследователями было показано, что DCTCP быстрее достигает сходимости к справедливому разделению пропускной способности(см.~рис~\ref{fair_share}), для этого они последовательно запускали 6 потоков, после чего останавливали их.
\begin{figure}
	\includegraphics[width=\linewidth]{fair_share}
	\caption{Тест сходимости DCTCP и TCP}
	\label{fair_share}
\end{figure}

Так же исследователи показали пакеты в DCTCP, передаются с куда меньшей задержкой в очереди, и доля пакетов, отправленных по тайм-ауту, равняется нулю вплоть до 35 отправителей (см.~рис~\ref{delays}).

\begin{figure}
	\includegraphics[width=\linewidth]{delays1}
	\caption{Длина очереди для DCTCP и TCP. Доля пакетов, отправленных по тайм-ауту}
	\label{delays}
\end{figure}

Создатели протокола D\textsuperscript{3} показали, что их протокол обеспечивает большую \emph{пропускную способность приложений} (скорость передачи, при условии, что передающий их поток, успел завершиьтся в свой директивный интервал) по сравнению с протоколами TCP, TCP\textsubscript{pr} и RCP\textsubscript{pr}~(см.~рис.\ref{d3_throughput}).  
\begin{figure}
	\includegraphics[width=\linewidth]{d3_throughput}
	\caption{Пропускная способность приложений для разного количество независимых отправителей с директивными интервалами}
	\label{d3_throughput}
\end{figure}

Протокол D\textsuperscript{2}TCP является обобщение протокола является обобщением, протокола DCTCP для работы с потоками, имеющими директивные интервалы, и, следственно, к нему справедливо наследует преимущества DCTCP. При этом исследователи показали, что с помощью D\textsuperscript{2}TCP меньше потоков не уместились в свои директивные интервалы по сравнению в протоколами TCP, DCTCP и D\textsuperscript{3} как без фонового трафика (см.~рис.~\ref{d2_missed_deadlines}), так и с фоновым трафиком (см.~рис.~\ref{d2_back_missed_deadlines}), при этом пропускная способность фонового трафика остается высокой (см.~рис.~\ref{d2_back_flows}).
\begin{figure}
	\includegraphics[width=\linewidth]{d2_missed_deadlines}
	\caption{Процент потоков, не уложившихся в свои директивные интервалы в зависимости от количества отправителей}
	\label{d2_missed_deadlines}
\end{figure}

\begin{figure}
	\includegraphics[width=\linewidth]{d2_back_missed_deadlines}
	\caption{Процент потоков, не уложившихся в свои директивные интервалы в зависимости от количества отправителей, при фоновом трафике}
	\label{d2_back_missed_deadlines}
\end{figure}

\begin{figure}
	\includegraphics[width=\linewidth]{d2_back_flows}
	\caption{Пропускная способность фонового трафика в зависимости от количества отправителей}
	\label{d2_back_flows}
\end{figure}
\newpage

\section{Заключение}

Центры обработки данных имеют специфичные проблемы, связанные с перегрузками сети, при этом приложениям предъявляют высокие требования к этой сети. В силу этого, классические алгоритмы борьбы с перегрузками не показывают подходящий результат для нужного качества работы приложений в центрах обработки данных. Поэтому исследователи адаптируют и создают новые алгоритмы управления перегрузками к работе в центрах обработки данных.

В данной работе рассмотрены алгоритмы управления перегрузками в центрах обработки данных на примере трех протоколов:
\begin{enumerate}
\item DCTCP использует схему маркирования пакетов на ранних этапах перегрузок, что позволяет отправителям определять степень перегруженности сети и плавно менять скорость передачи данных, приходя к равновесию,
\item D\textsuperscript{3} использует знания о директивных интервалах потоков, отправители резервируют пропускную способность на маршрутизаторах в зависимости от количества данных, которые им необходимо передать, и близости окончания директивного интервала,
\item D\textsuperscript{2}TCP использует схему DCTCP, при этом позволяя отправителям, окончания директивного интервала которых достаточно скоро, меньше уменьшать скорость передачи в случае перегрузки, и наоборот, заставляя отправителей, чье окончание директивного интервала наступит не скоро, сильнее уменьшать скорость при перегрузке. 
\end{enumerate}

Стоит заметить, что эти протоколы целесообразно применять только в центрах обработки данных, где существует строго определенный трафик и известна топология. Но это не уменьшает важность этих протоколов. На сегодняшний день центры обработки данных это повсеместное явление, все сервисы, которые обрабатывают большие объемы информации не могут обойтись без центров обработки данных (веб-поиск, социальные сети, хостинг видео и музыки). На базе центров обработки данных создают облачные сервисы, что позволяет некоторым компаниям не поддерживать свою серверную-инфраструктуру.

\newpage
\begin{thebibliography}{9}
\addcontentsline{toc}{section}{\bibname}

\bibitem {dctcp}
Alizadeh M., Greenberg A., Maltz
 D.A., Padhye J., Patel P., Prabhakar B., 
Sengupta S., and Sridharan M. Data Center TCP (DCTCP) // ACM SIGCOMM 
Computer Communication Review -
 SIGCOMM '10, Volume 40 Issue 4. Oct. 
2010. Pp. 63
-74.

\bibitem {d3tcp}
Wilson C., Ballani H., Karagiannis T., Rowtron
 A. Better never than late: meeting 
deadlines in datacenter networks // ACM SIGCOMM Computer Communication 
Review 
- SIGCOMM '11 Volume 41 Issue 4. Aug. 2011. Pp. 50
-61. 

\bibitem {d2tcp}
Vamanan B., Hasan J., Vijaykumar T.N. Deadline
-aware datacenter tcp (D2TCP) 
// Proceedi
ngs of the ACM SIGCOMM 2012 conference on Applications, 
technologies, architectures, and protocols for computer communication. Aug. 
2012. Pp. 115
-126. 

\bibitem {facebook}
Nishtala R. et al. Scaling Memcache at Facebook //nsdi. – 2013. – Т. 13. – С. 385-398.
\bibitem {sla}
DeCandia G. et al. Dynamo: amazon's highly available key-value store //ACM SIGOPS operating systems review. – 2007. – Т. 41. – №. 6. – С. 205-220.
\bibitem {jittering}
Floyd S., Jacobson V. The synchronization of periodic routing messages //IEEE/ACM transactions on networking. – 1994. – Т. 2. – №. 2. – С. 122-136.
\bibitem {ictcp}
Wu H. et al. ICTCP: Incast congestion control for TCP in data-center networks //IEEE/ACM transactions on networking. – 2013. – Т. 21. – №. 2. – С. 345-358.
\bibitem {red}
Floyd S., Jacobson V. Random early detection gateways for congestion avoidance //IEEE/ACM Transactions on Networking (ToN). – 1993. – Т. 1. – №. 4. – С. 397-413.
\bibitem {edf}
Liu C. L., Layland J. W. Scheduling algorithms for multiprogramming in a hard-real-time environment //Journal of the ACM (JACM). – 1973. – Т. 20. – №. 1. – С. 46-61.
\bibitem {np-complete} 
Chen B. B., Primet P. V. B. Scheduling deadline-constrained bulk data transfers to minimize network congestion //Cluster Computing and the Grid, 2007. CCGRID 2007. Seventh IEEE International Symposium on. – IEEE, 2007. – С. 410-417.

\end{thebibliography}

\end{document}